---
title: "Real Attackers Don`t Compute Gradients: Bridging the Gap Between Adversarial ML Research and Practice"
collection: publications
permalink: /publications/satml23
excerpt: "Let's change the domain of adversarial ML. For real."
date: 2023-02-08
code: '[SaTML23]'
venue: 'IEEE Conference on Secure and Trustworthy Machine Learning'
badge: <span class='badge badge-primary'>Conference</span>
type: Conference
authors: '<u>Apruzzese, G.</u>, Anderson, H. S., Dambra, S., Freeman, D., Pierazzi, F., & Roundy, K. A.'
citation: '<u>Apruzzese, G.</u>, Anderson, H. S., Dambra, S., Freeman, D., Pierazzi, F., & Roundy, K. A. (2023, Feb). "Real Attackers Don`t Compute Gradients: Bridging the Gap Between Adversarial ML Research and Practice" In <i>2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)</i>.'
---
{% include base_path %}
<b>Abstract.</b> Recent years have seen a proliferation of research on _adversarial machine learning_.
Numerous papers demonstrate powerful algorithmic attacks against a wide variety of machine learning (ML) models, and numerous other papers propose defenses that can withstand most attacks. 
However, abundant real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems, and as a result security practitioners have not prioritized adversarial ML defenses.

Motivated by the apparent gap between researchers and practitioners, this position paper aims to bridge these two domains. 
We first present three real-world case studies from which we can glean practical insights unknown or neglected in research.
Next, we analyze all adversarial ML papers recently published in top security conferences and highlight positive trends and blind spots. 
Finally, we state positions on precise and cost-driven threat modeling, collaboration between industry and academia, and reproducible research. 
If adopted, our positions will increase the real-world impact of future endeavours in adversarial ML, bringing both researchers and practitioners closer to their shared goal of improving the security of ML systems.




<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="{{ base_path }}/files/papers/satml23/satml23.pdf" target="_blank" rel="noopener">Paper PDF</a> 
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="{{ base_path }}/files/papers/satml23/satml23_cite.html" target="_blank" rel="noopener">Cite</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/forum?id=54Jcj2YmJg" target="_blank" rel="noopener">OpenReview</a> <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://real-gradients.github.io" target="_blank" rel="noopener">Website</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="{{ base_path }}/talks/satml23" target="_blank" rel="noopener">Talk</a> 
